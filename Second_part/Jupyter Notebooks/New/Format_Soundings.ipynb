{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format the soundings and save variables of interest in Tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Tables/data-0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Tables/data-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     42\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHGHT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRES\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDWPT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDIR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 43\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_table_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%0.2f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Print the table for this timeframe\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#table_str = tabulate(data_table_i, headers=headers, tablefmt='plain')\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#print('Table for timeframe {}:'.format(time_str))\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#print(table_str)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omitu\\anaconda3\\envs\\metstat\\lib\\site-packages\\numpy\\lib\\npyio.py:1556\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_string_like(fname):\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;66;03m# datasource doesn't support creating a new file ...\u001b[39;00m\n\u001b[1;32m-> 1556\u001b[0m     \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1557\u001b[0m     fh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m   1558\u001b[0m     own_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Tables/data-0.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from metpy.units import units\n",
    "from metpy.calc import mixing_ratio\n",
    "from tabulate import tabulate\n",
    "\n",
    "data = xr.open_dataset('C:/Users/omitu/Documents/GitHub/Urbanization-and-Climate-Change/Second_part/data/sounding/houinterpolatedsondeM1.c1.20220810.000030.nc')\n",
    "\n",
    "# Extract the variables needed for the sounding\n",
    "p = (data['bar_pres'][:, :] * 10.0)\n",
    "rh = (data['rh'][:, :]) \n",
    "T = data['temp'][:, :] \n",
    "Td = data['dp'][:, :]\n",
    "wdir = data['wdir'][:, :]\n",
    "wspd = (data['wspd'][:, :])\n",
    "height = (data['bar_pres']['height'].values * 1000.0)\n",
    "precip = data['precip'][:]\n",
    "\n",
    "# Calculate additional variables needed for the sounding\n",
    "#w = mixing_ratio(p, Td)\n",
    "\n",
    "# Loop over all timeframes and save the tables with the appropriate names\n",
    "for i in range(len(data['time'])):\n",
    "    # Extract the variables for this timeframe\n",
    "    p_i = p[i, :]\n",
    "    rh_i = rh[i, :]\n",
    "    T_i = T[i, :]\n",
    "    Td_i = Td[i, :]\n",
    "    wdir_i = wdir[i, :]\n",
    "    wspd_i = wspd[i, :]\n",
    "    \n",
    "    # Combine the data into a table\n",
    "    data_table_i = np.vstack((height, p_i, T_i, Td_i, rh_i, wdir_i, wspd_i)).T\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    mask_i = np.isnan(data_table_i).any(axis=1)\n",
    "    data_table_i = data_table_i[~mask_i]\n",
    "    \n",
    "    # Save the modified data table as a txt file with the appropriate name\n",
    "    #time_str = data['time'][i].strftime('%H%M%S')\n",
    "    filename = './Tables/data-' + str(i) + '.txt'\n",
    "    headers = ['HGHT', 'PRES', 'TEMP', 'DWPT', 'RH', 'DIR', 'SPD']\n",
    "    np.savetxt(filename, data_table_i, header='\\t'.join(headers), fmt='%0.2f', delimiter='\\t')\n",
    "    \n",
    "    # Print the table for this timeframe\n",
    "    #table_str = tabulate(data_table_i, headers=headers, tablefmt='plain')\n",
    "    #print('Table for timeframe {}:'.format(time_str))\n",
    "    #print(table_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate various sounding parameters for all the data in Tables folder and save the output to Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 MetPy Developers.\n",
    "# Distributed under the terms of the BSD 3-Clause License.\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\"\"\"\n",
    "=============================\n",
    "Sounding Calculation Examples\n",
    "=============================\n",
    "\n",
    "Use functions from `metpy.calc` to perform a number of calculations using sounding data.\n",
    "\n",
    "The code below uses example data to perform many sounding calculations for a severe weather\n",
    "event on May 22, 2011 from the Norman, OK sounding.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.units import units\n",
    "\n",
    "###########################################\n",
    "# Effective Shear Algorithm for use in Supercell Composite Calculation\n",
    "\n",
    "\n",
    "def effective_layer(p, t, td, h, height_layer=False):\n",
    "    \"\"\"A function that determines the effective inflow layer for a convective sounding.\n",
    "\n",
    "    Uses the default values of Thompason et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg).\n",
    "\n",
    "    Input:\n",
    "      - p: sounding pressure with units\n",
    "      - T: sounding temperature with units\n",
    "      - Td: sounding dewpoint temperature with units\n",
    "      - h: sounding heights with units\n",
    "\n",
    "    Returns:\n",
    "      - pbot/hbot, ptop/htop: pressure/height of the bottom level,\n",
    "                              pressure/height of the top level\n",
    "    \"\"\"\n",
    "    from metpy.calc import cape_cin, parcel_profile\n",
    "    from metpy.units import units\n",
    "\n",
    "    pbot = None\n",
    "\n",
    "    for i in range(p.shape[0]):\n",
    "        prof = parcel_profile(p[i:], t[i], td[i])\n",
    "        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n",
    "        if sbcape >= 100 * units('J/kg') and sbcin > -250 * units('J/kg'):\n",
    "            pbot = p[i]\n",
    "            hbot = h[i]\n",
    "            bot_idx = i\n",
    "            break\n",
    "    if not pbot:\n",
    "        return None, None\n",
    "\n",
    "    for i in range(bot_idx + 1, p.shape[0]):\n",
    "        prof = parcel_profile(p[i:], t[i], td[i])\n",
    "        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n",
    "        if sbcape < 100 * units('J/kg') or sbcin < -250 * units('J/kg'):\n",
    "            ptop = p[i]\n",
    "            htop = h[i]\n",
    "            break\n",
    "\n",
    "    if height_layer:\n",
    "        return hbot, htop\n",
    "    else:\n",
    "        return pbot, ptop\n",
    "\n",
    "\n",
    "###########################################\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "rows = []\n",
    "# Loop over all the files from data-0 to data-1439\n",
    "for i in range(1440):\n",
    "    # Construct the filename for the current file\n",
    "    filename = f'data-{i}.txt'\n",
    "    filepath = os.path.join('./Tables/', filename)\n",
    "\n",
    "    # Read the text file into a pandas DataFrame\n",
    "    col_names = ['height', 'pressure', 'temperature', 'dewpoint', 'rh', 'direction', 'speed']\n",
    "    df = pd.read_csv(filepath, delim_whitespace=True, skiprows=2,  names=col_names)\n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    # Isolate needed variables from our data file and attach units\n",
    "    p = df['pressure'].values * units.hPa\n",
    "    T = df['temperature'].values * units.degC\n",
    "    Td = df['dewpoint'].values * units.degC\n",
    "    wdir = df['direction'].values * units.degree\n",
    "    sped = df['speed'].values * units.knot\n",
    "    height = df['height'].values * units.meter\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the wind components\n",
    "    u, v = mpcalc.wind_components(sped, wdir)\n",
    "\n",
    "    ###########################################\n",
    "    # Compute common sounding index parameters\n",
    "    ctotals = mpcalc.cross_totals(p, T, Td)\n",
    "    kindex = mpcalc.k_index(p, T, Td)\n",
    "    showalter = mpcalc.showalter_index(p, T, Td)\n",
    "    total_totals = mpcalc.total_totals_index(p, T, Td)\n",
    "    vert_totals = mpcalc.vertical_totals(p, T)\n",
    "\n",
    "    ###########################################\n",
    "    # Compture the parcel profile for a surface-based parcel\n",
    "    prof = mpcalc.parcel_profile(p, T[0], Td[0])\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the corresponding LI, CAPE, CIN values for a surface parcel\n",
    "    lift_index = mpcalc.lifted_index(p, T, prof)\n",
    "    cape, cin = mpcalc.cape_cin(p, T, Td, prof)\n",
    "\n",
    "    ###########################################\n",
    "    # Determine the LCL, LFC, and EL for our surface parcel\n",
    "    lclp, lclt = mpcalc.lcl(p[0], T[0], Td[0])\n",
    "    lfcp, _ = mpcalc.lfc(p, T, Td)\n",
    "    el_pressure, _ = mpcalc.el(p, T, Td, prof)\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the characteristics of a mean layer parcel (50-hPa depth)\n",
    "    ml_t, ml_td = mpcalc.mixed_layer(p, T, Td, depth=50 * units.hPa)\n",
    "    ml_p, _, _ = mpcalc.mixed_parcel(p, T, Td, depth=50 * units.hPa)\n",
    "    mlcape, mlcin = mpcalc.mixed_layer_cape_cin(p, T, prof, depth=50 * units.hPa)\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the characteristics of the most unstable parcel (50-hPa depth)\n",
    "    mu_p, mu_t, mu_td, _ = mpcalc.most_unstable_parcel(p, T, Td, depth=50 * units.hPa)\n",
    "    mucape, mucin = mpcalc.most_unstable_cape_cin(p, T, Td, depth=50 * units.hPa)\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the Bunkers Storm Motion vector and use to calculate the critical angle\n",
    "    (u_storm, v_storm), *_ = mpcalc.bunkers_storm_motion(p, u, v, height)\n",
    "    critical_angle = mpcalc.critical_angle(p, u, v, height, u_storm, v_storm)\n",
    "\n",
    "    ###########################################\n",
    "    # Work on the calculations needed to compute the significant tornado parameter\n",
    "\n",
    "    # Estimate height of LCL in meters from hydrostatic thickness\n",
    "    new_p = np.append(p[p > lclp], lclp)\n",
    "    new_t = np.append(T[p > lclp], lclt)\n",
    "    lcl_height = mpcalc.thickness_hydrostatic(new_p, new_t)\n",
    "\n",
    "    # Compute Surface-based CAPE\n",
    "    sbcape, _ = mpcalc.surface_based_cape_cin(p, T, Td)\n",
    "\n",
    "    # Compute SRH, given a motion vector toward the NE at 9.9 m/s\n",
    "    *_, total_helicity = mpcalc.storm_relative_helicity(height, u, v, depth=1 * units.km,\n",
    "                                                        storm_u=u_storm, storm_v=v_storm)\n",
    "\n",
    "    # Copmute Bulk Shear components and then magnitude\n",
    "    ubshr, vbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=6 * units.km)\n",
    "    bshear = mpcalc.wind_speed(ubshr, vbshr)\n",
    "\n",
    "    # Use all computed pieces to calculate the Significant Tornado parameter\n",
    "    sig_tor = mpcalc.significant_tornado(sbcape, lcl_height,\n",
    "                                        total_helicity, bshear).to_base_units()\n",
    "\n",
    "    ###########################################\n",
    "    # Compute the supercell composite parameter, if possible\n",
    "\n",
    "    # Determine the top and bottom of the effective layer using our own function\n",
    "    hbot, htop = effective_layer(p, T, Td, height, height_layer=True)\n",
    "\n",
    "    # Perform the calculation of supercell composite if an effective layer exists\n",
    "    if hbot:\n",
    "        esrh = mpcalc.storm_relative_helicity(height, u, v, depth=htop - hbot, bottom=hbot)\n",
    "        eubshr, evbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=htop - hbot, bottom=hbot)\n",
    "        ebshear = mpcalc.wind_speed(eubshr, evbshr)\n",
    "\n",
    "        super_comp = mpcalc.supercell_composite(mucape, esrh[0], ebshear)\n",
    "    else:\n",
    "        super_comp = np.nan\n",
    "\n",
    "    \n",
    "    # Create a dictionary of parameter names and values\n",
    "    params = {\n",
    "        'CAPE': [cape.magnitude, str(cape.units)],\n",
    "        'CIN': [cin.magnitude, str(cin.units)],\n",
    "        'LCL Pressure': [lclp.magnitude, str(lclp.units)],\n",
    "        'LFC Pressure': [lfcp.magnitude, str(lfcp.units)],\n",
    "        'EL Pressure': [el_pressure.magnitude, str(el_pressure.units)],\n",
    "        'Lifted Index': [lift_index.magnitude, str(lift_index.units)],\n",
    "        'K-Index': [kindex.magnitude, str(kindex.units)],\n",
    "        'Showalter Index': [showalter.magnitude, str(showalter.units)],\n",
    "        'Cross Totals': [ctotals.magnitude, str(ctotals.units)],\n",
    "        'Total Totals': [total_totals.magnitude, str(total_totals.units)],\n",
    "        'Vertical Totals': [vert_totals.magnitude, str(vert_totals.units)],\n",
    "        'Mixed Layer - Lowest 50-hPa Temp': [ml_t.magnitude, str(ml_t.units)],\n",
    "        'Mixed Layer - Lowest 50-hPa Dewp': [ml_td.magnitude, str(ml_td.units)],\n",
    "        'Mixed Layer - Lowest 50-hPa CAPE': [mlcape.magnitude, str(mlcape.units)],\n",
    "        'Mixed Layer - Lowest 50-hPa CIN': [mlcin.magnitude, str(mlcin.units)],\n",
    "        'Most Unstable - Lowest 50-hPa Temp': [mu_t.magnitude, str(mu_t.units)],\n",
    "        'Most Unstable - Lowest 50-hPa Dewp': [mu_td.magnitude, str(mu_td.units)],\n",
    "        'Most Unstable - Lowest 50-hPa Pressure': [mu_p.magnitude, str(mu_p.units)],\n",
    "        'Most Unstable - Lowest 50-hPa CAPE': [mucape.magnitude, str(mucape.units)],\n",
    "        'Most Unstable - Lowest 50-hPa CIN': [mucin.magnitude, str(mucin.units)],\n",
    "        'Bunkers Storm Motion Vector - u_storm': [u_storm.magnitude, str(u_storm.units)],\n",
    "        'Bunkers Storm Motion Vector - v_storm': [v_storm.magnitude, str(v_storm.units)],\n",
    "        'Critical Angle': [critical_angle.magnitude, str(critical_angle.units)],\n",
    "        'Storm Relative Helicity': [total_helicity.magnitude, str(total_helicity.units)],\n",
    "        'Significant Tornado Parameter': [sig_tor.magnitude, str(sig_tor.units)],\n",
    "        'Supercell Composite Parameter': [super_comp.magnitude, str(super_comp.units)],\n",
    "    }\n",
    "    \n",
    "    # Create a new row with different values\n",
    "    new_row = '\\t'.join([str(value[0]) for value in params.values()])\n",
    "    # Append the new row to the rows list\n",
    "    rows.append(new_row)\n",
    "\n",
    "# Write the header row and all the data rows to a file\n",
    "with open('Output.txt', 'w') as f:\n",
    "    # Write the header row\n",
    "    f.write('\\t'.join(params.keys()) + '\\n')\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in rows:\n",
    "        f.write(row + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load Output.txt as a dataframe\n",
    "output_df = pd.read_csv('Output.txt', sep='\\t')\n",
    "\n",
    "\n",
    "# Loop through data-0 to data-1439 files\n",
    "for i in range(1,1440):\n",
    "    # Construct the filename\n",
    "    filename = f'./Tables/data-{i}.txt'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # Load the data file as a dataframe\n",
    "        data_df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Append the row from Output.txt to the beginning of the data dataframe\n",
    "        data_df = pd.concat([output_df.iloc[i:i+1], data_df], axis=1)\n",
    "        \n",
    "        # Save the modified data file\n",
    "        data_df.to_csv(filename, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
